{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `ELAN_Data`\n",
    "***\n",
    "[Alejandro Ciuba](https://alejandrociuba.github.io), alejandrociuba@pitt.edu\n",
    "***\n",
    "## Introduction\n",
    "This is the first of several notebooks which will get the reader familiarized with the Python package `ELAN_Data` and its various capabilities. For these tutorials, we will utilize the Corpus of Regional African American Language ([CORAAL](http://lingtools.uoregon.edu/coraal/)); specifically, we will use the `ATL_elanfiles_2020.05.tar.gz`. Please download and unzip this dataset into the dataset folder.\n",
    "\n",
    "## Table of Contents\n",
    "1. [The Basics](#the-basics)\n",
    "2. Loading in Files\n",
    "3. `ELAN_Data`\n",
    "4. Advanced `ELAN_Data`\n",
    "5. `elan_utils`\n",
    "***\n",
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elan_data import ELAN_Data\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## The Basics\n",
    "The `elan_data` package is made of two primary Python files structured as follows:\n",
    "\n",
    "```\n",
    "src\n",
    "└── elan_data\n",
    "    ├── elan_utils.py\n",
    "    ├── __init__.py\n",
    "    └── py.typed\n",
    "```\n",
    "\n",
    "### Files\n",
    "| Name | Description |\n",
    "| :--- | ----------- |\n",
    "| `elan_utils.py` | Contains helper methods that work with `ELAN_Data` and are planned to be adaptable to be more general purpose tools as well. |\n",
    "| `__init__.py` | Contains the main `ELAN_Data` code. |\n",
    "| `py.typed` | This literally does nothing but is needed so our testing framework won't be mad. >:( |\n",
    "\n",
    "### Simple Code\n",
    "To start using `elan_data` simply import its main module, `ELAN_Data` and either load-in or create an instance. To load-in a pre-existing file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to .eaf file is datasets/ATL_se0_ag1_f_01_1.eaf\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18643/3352804807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load in as an ELAN_Data instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0meaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mELAN_Data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programs/Python_Programs/elan_data/src/elan_data/__init__.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, file, init_df)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mdescriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".//*[@MIME_TYPE]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0med_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MEDIA_URL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load in ATL_se0_ag1_f_01_1.eaf from the downloaded dataset\n",
    "# We recommend using Path to store and modify file paths; it's an official Python package and very robust\n",
    "FILE = \"./datasets/ATL_se0_ag1_f_01_1.eaf\"\n",
    "PATH = Path(FILE)\n",
    "\n",
    "print(f\"Path to .eaf file is {PATH}\")\n",
    "\n",
    "# Load in as an ELAN_Data instance\n",
    "eaf = ELAN_Data.from_file(file=PATH)\n",
    "\n",
    "print(eaf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elan_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
